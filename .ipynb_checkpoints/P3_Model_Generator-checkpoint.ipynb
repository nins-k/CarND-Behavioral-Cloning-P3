{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set paths for the training data\n",
    "\n",
    "# base_dir = 'C:\\\\Users\\\\nins_\\\\Desktop\\\\Data3\\\\'\n",
    "base_dir = '/home/carnd/Data/'\n",
    "img_dir = \"IMG/\"\n",
    "csv_file_name = ['driving_log.csv', 'driving_log2.csv', 'driving_log3.csv', 'driving_log4.csv', 'driving_log5.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Convolution2D, MaxPooling2D, Activation, Cropping2D, Dropout, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv and images into training and label numpy arrays\n",
    "df1 = pd.read_csv(base_dir+csv_file_name[0], header=None)\n",
    "df2 = pd.read_csv(base_dir+csv_file_name[1], header=None)\n",
    "df3 = pd.read_csv(base_dir+csv_file_name[2], header=None)\n",
    "df4 = pd.read_csv(base_dir+csv_file_name[3], header=None)\n",
    "df = pd.concat([df1, df2, df3, df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "data_train, data_valid = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator for supplying data to the model in batches\n",
    "\n",
    "def data_generator(data, batch_size, angle_bias=0.25):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # Shuffle the data before each epoch\n",
    "        data = shuffle(data)\n",
    "        \n",
    "        # Obtain samples equal to batch size and process the images\n",
    "        for offset in range(0, total_samples, batch_size):\n",
    "            batch = data[offset:batch_size+offset]\n",
    "            \n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            # Load images and labels equal to the batch size only\n",
    "            for row in batch:\n",
    "                \n",
    "                img_name = row[0].split(\"\\\\\")[-1]\n",
    "                img_C = cv2.cvtColor(cv2.imread(base_dir+img_dir+img_name), cv2.COLOR_BGR2RGB)\n",
    "                label_C = float(row[3])\n",
    "                \n",
    "                img_name = row[1].split(\"\\\\\")[-1]\n",
    "                img_L = cv2.cvtColor(cv2.imread(base_dir+img_dir+img_name), cv2.COLOR_BGR2RGB)\n",
    "                label_L = label_C + angle_bias\n",
    "                \n",
    "                img_name = row[2].split(\"\\\\\")[-1]\n",
    "                img_R = cv2.cvtColor(cv2.imread(base_dir+img_dir+img_name), cv2.COLOR_BGR2RGB)\n",
    "                label_R = label_C - angle_bias\n",
    "                \n",
    "                # Append to the batch data\n",
    "                batch_images.append(img_C)\n",
    "                batch_images.append(img_L)\n",
    "                batch_images.append(img_R)\n",
    "                \n",
    "                batch_labels.append(label_C)\n",
    "                batch_labels.append(label_L)\n",
    "                batch_labels.append(label_R)\n",
    "                \n",
    "                # Flip the image\n",
    "                flip_img_C = cv2.flip(img_C, 1)\n",
    "                flip_label_C = label_C * (-1)\n",
    "                \n",
    "                flip_img_L = cv2.flip(img_L, 1)\n",
    "                flip_label_L = label_L * (-1)\n",
    "                \n",
    "                flip_img_R = cv2.flip(img_R, 1)\n",
    "                flip_label_R = label_R * (-1)\n",
    "                \n",
    "                # Append the augmented data to the batch data\n",
    "                batch_images.append(flip_img_C)\n",
    "                batch_images.append(flip_img_L)\n",
    "                batch_images.append(flip_img_R)\n",
    "                \n",
    "                batch_labels.append(flip_label_C)\n",
    "                batch_labels.append(flip_label_L)\n",
    "                batch_labels.append(flip_label_R)\n",
    "                \n",
    "                          \n",
    "            X = np.array(batch_images)\n",
    "            y = np.array(batch_labels)\n",
    "\n",
    "            yield shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create generators for training and validation\n",
    "\n",
    "train_generator = data_generator(data_train, batch_size=32)\n",
    "valid_generator = data_generator(data_valid, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_7 (Lambda)                (None, 160, 320, 3)   0           lambda_input_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_12 (Cropping2D)       (None, 90, 320, 3)    0           lambda_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 86, 316, 8)    608         cropping2d_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_34 (MaxPooling2D)   (None, 43, 158, 8)    0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 43, 158, 8)    0           maxpooling2d_34[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 41, 156, 6)    438         activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_35 (MaxPooling2D)   (None, 20, 78, 6)     0           convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 20, 78, 6)     0           maxpooling2d_35[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_36 (Convolution2D) (None, 20, 78, 6)     330         activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_36 (MaxPooling2D)   (None, 10, 39, 6)     0           convolution2d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 10, 39, 6)     0           maxpooling2d_36[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 2340)          0           activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_45 (Dense)                 (None, 256)           599296      flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 256)           0           dense_45[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_46 (Dense)                 (None, 128)           32896       dropout_29[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (None, 128)           0           dense_46[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_47 (Dense)                 (None, 64)            8256        dropout_30[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 64)            0           dense_47[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (None, 1)             65          dropout_31[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 641,889\n",
      "Trainable params: 641,889\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "\n",
    "model.add(Convolution2D(8, 5, 5, border_mode='valid', subsample=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Convolution2D(6, 3, 3, border_mode='valid', subsample=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Convolution2D(6, 3, 3, border_mode='same', subsample=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam_opt = Adam(lr=0.005)\n",
    "model.compile(loss='mse', optimizer=adam_opt)\n",
    "filepath=\"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4608/4670 [============================>.] - ETA: 0s - loss: 0.1109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4670 [==============================] - 11s - loss: 0.1087 - val_loss: 0.0647\n",
      "Epoch 2/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0577 - val_loss: 0.0650\n",
      "Epoch 3/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0584 - val_loss: 0.0616\n",
      "Epoch 4/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0571 - val_loss: 0.0589\n",
      "Epoch 5/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0652 - val_loss: 0.0667\n",
      "Epoch 6/40\n",
      "4788/4670 [==============================] - 10s - loss: 0.0550 - val_loss: 0.0586\n",
      "Epoch 7/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0547 - val_loss: 0.0474\n",
      "Epoch 8/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0486 - val_loss: 0.0330\n",
      "Epoch 9/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0418 - val_loss: 0.0353\n",
      "Epoch 10/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0367 - val_loss: 0.0377\n",
      "Epoch 11/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0350 - val_loss: 0.0350\n",
      "Epoch 12/40\n",
      "4788/4670 [==============================] - 10s - loss: 0.0331 - val_loss: 0.0273\n",
      "Epoch 13/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0309 - val_loss: 0.0273\n",
      "Epoch 14/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0300 - val_loss: 0.0318\n",
      "Epoch 15/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0311 - val_loss: 0.0323\n",
      "Epoch 16/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0288 - val_loss: 0.0256\n",
      "Epoch 17/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0310 - val_loss: 0.0327\n",
      "Epoch 18/40\n",
      "4788/4670 [==============================] - 10s - loss: 0.0287 - val_loss: 0.0299\n",
      "Epoch 19/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0293 - val_loss: 0.0291\n",
      "Epoch 20/40\n",
      "4800/4670 [==============================] - 10s - loss: 0.0253 - val_loss: 0.0289\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit_generator(train_generator, callbacks=[checkpoint, earlystop], samples_per_epoch=len(data_train), validation_data=valid_generator, nb_val_samples=len(data_valid), nb_epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
